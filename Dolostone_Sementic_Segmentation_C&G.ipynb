{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dolostone_Sementic_Segmentation_C&G.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uSx_o7BP0aG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('path')\n",
        "print('Mounted.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taAKzk0QP2me"
      },
      "source": [
        "epochs = 50\n",
        "size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe5Lb_5fP5Bx"
      },
      "source": [
        "!pip install gdal patchify tensorflow_addons geojson shapely geopandas rasterio==1.2.7 fiona\n",
        "import rasterio\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.applications import VGG16, InceptionResNetV2, ResNet50, Xception, InceptionV3\n",
        "import rasterio.plot\n",
        "import geopandas as gpd\n",
        "import rasterio.mask\n",
        "import random\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shapely\n",
        "import gdal\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import skimage\n",
        "import patchify\n",
        "import rasterio.plot\n",
        "import gc\n",
        "import random\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn import utils\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, ZeroPadding2D\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORlpaTIWP8wg"
      },
      "source": [
        "def readimage():\n",
        "  image = gdal.Open(\"path").ReadAsArray()\n",
        "  image = rasterio.plot.reshape_as_image(image)\n",
        "  return image\n",
        "image = readimage()\n",
        "#image = image.astype('uint8')\n",
        "image = image[:,:,0:3]\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDKB8M3JQAKv"
      },
      "source": [
        "def readgeo(link):\n",
        "  geo = gpd.read_file(link)\n",
        "  print(geo.crs)\n",
        "  with rasterio.open(\"path") as src:\n",
        "    print(src.crs)\n",
        "    raster = src.read()\n",
        "    geo = geo.to_crs(src.crs)\n",
        "    #geo.plot()\n",
        "    out_image, out_transform = rasterio.mask.mask(src, geo.geometry, filled=True)\n",
        "    masks = out_image[3,:,:]\n",
        "    masks[np.where(masks<=0)] = 0\n",
        "    masks[np.where(masks>0)] = 1\n",
        "    masks = masks.astype(np.int8)\n",
        "    print(masks.shape)\n",
        "    dolo_pixels = str(np.unique(masks, return_counts=True)[1][1])\n",
        "    return masks, dolo_pixels\n",
        "    \n",
        "masks, dolo_pixels = readgeo(\"path\")\n",
        "masks_limestone, limestone_pixels = readgeo(\"path")\n",
        "masks_background, background_pixels = readgeo(\"path")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT_abhr_QWxN"
      },
      "source": [
        "def combining():\n",
        "  a = np.expand_dims(masks, axis=2)\n",
        "  b = np.expand_dims(masks_limestone, axis=2)\n",
        "  c = np.expand_dims(masks_background, axis=2)\n",
        "  y_train = np.concatenate((a,b,c), axis=2)\n",
        "  return y_train\n",
        "y_train = combining()\n",
        "y_train = y_train.astype('uint8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6JT2oavQddT"
      },
      "source": [
        "y_train = skimage.util.view_as_windows(y_train, (size, size, 3), step = int(size/1.4))\n",
        "y_train = y_train.reshape((-1,) + (size, size, 3))\n",
        "\n",
        "def tiling():\n",
        "  patch_arr = skimage.util.view_as_windows(image, (size, size, 3), step = int(size/1.4))\n",
        "  output = patch_arr.reshape((-1,) + (size, size, 3))\n",
        "  return output\n",
        "image = tiling()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTprW8lpQn1o"
      },
      "source": [
        "#undersampling\n",
        "indexes = []\n",
        "for i in np.arange(0, len(y_train)):\n",
        "  if np.sum(y_train[i,:,:,2]) >= int(size * size * 0.99):\n",
        "    indexes.append(i)\n",
        "  else:\n",
        "    continue\n",
        "random.shuffle(indexes)\n",
        "indexes = indexes[: int(len(indexes) / 2)]\n",
        "original = np.arange(0, len(y_train))\n",
        "revised = np.delete(original, indexes)\n",
        "y_train = y_train[revised, :, :, :]\n",
        "image = image[revised, :, :, :]\n",
        "\n",
        "#Oversampling lithology boundaries for dolomite\n",
        "indexes = []\n",
        "for i in np.arange(0, len(y_train)):\n",
        "  if np.sum(y_train[i,:,:,0]) >= int(size * size * 0.33):\n",
        "    indexes.append(i)\n",
        "  else:\n",
        "    continue\n",
        "import random\n",
        "random.shuffle(indexes)\n",
        "indexes = indexes[: int(len(indexes) / 10)]\n",
        "original = np.arange(0, len(y_train))\n",
        "print(len(original))\n",
        "aa = y_train[indexes, :, :, :]\n",
        "y_train = np.concatenate((aa, y_train), axis = 0)\n",
        "bb = image[indexes, :, :, :]\n",
        "image = np.concatenate((bb, image), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNqe-fRQ1QV"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(image, y_train, test_size=0.1, shuffle=True)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFSO4humQ7fp"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = True,\n",
        "    rotation_range = 45,\n",
        "    shear_range=25,\n",
        "    brightness_range=(0.5, 1.5),\n",
        "    validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maEEWMyCQ-uH"
      },
      "source": [
        "def iou_loss(y_true, y_pred):\n",
        "      y_true = tf.reshape(y_true, [-1])\n",
        "      y_pred = tf.reshape(y_pred, [-1])\n",
        "      intersection = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32))\n",
        "      score = (intersection + 1.) / (tf.reduce_sum(tf.cast(y_true, tf.float32)) + \n",
        "      tf.reduce_sum(tf.cast(y_pred, tf.float32)) - intersection + 1.)\n",
        "      return 1 - score\n",
        "def mean_iou(y_true, y_pred):\n",
        "        y_pred = tf.round(tf.cast(y_pred, tf.int32))\n",
        "        intersect = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32), axis=[1])\n",
        "        union = tf.reduce_sum(tf.cast(y_true, tf.float32),axis=[1]) + tf.reduce_sum(tf.cast(y_pred, tf.float32),axis=[1])\n",
        "        smooth = tf.ones(tf.shape(intersect))\n",
        "        return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
        "def custom_f1(y_true, y_pred):    \n",
        "    def recall_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        \n",
        "        recall = TP / (Positives+K.epsilon())    \n",
        "        return recall \n",
        "    \n",
        "    def precision_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    \n",
        "        precision = TP / (Pred_Positives+K.epsilon())\n",
        "        return precision \n",
        "    \n",
        "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
        "    \n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3eHc1F3RDOq"
      },
      "source": [
        "train_generator = datagen.flow(x_train, y_train, batch_size=16, subset='training')\n",
        "validation_generator = datagen.flow(x_train, y_train, batch_size=16, subset='validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoPnBqmyRIEp"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Td5QwdRRLL-"
      },
      "source": [
        "'''Xception'''\n",
        "def conv_block(inputs, num_filters):\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2D(num_filters, 2, padding='same')(s)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  x = Conv2D(num_filters, 2, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  return x\n",
        "\n",
        "def decoder(inputs, skip_features, num_filters):\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs)\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_xception_unit(input_shape, size):\n",
        "  '''Input'''\n",
        "  inputs = Input(input_shape)\n",
        "  '''Pretrained model'''\n",
        "  xception = Xception(include_top = False, weights = 'imagenet', input_tensor = inputs)\n",
        "  '''encoder'''\n",
        "  s1 = xception.get_layer(\"input_1\").output ##256\n",
        "\n",
        "  s2 = xception.get_layer(\"block1_conv1_act\").output ##127\n",
        "  s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)  #128\n",
        "\n",
        "  s3 = xception.get_layer(\"block3_sepconv2_bn\").output ##63\n",
        "  s3 = ZeroPadding2D(( (1, 0), (1, 0) ))(s3)  #64\n",
        "\n",
        "  s4 = xception.get_layer(\"block4_sepconv2_bn\").output ##32\n",
        "  '''bridge'''\n",
        "  b1 = xception.get_layer(\"block13_sepconv2_bn\").output ##16\n",
        "  '''decoder'''\n",
        "  d1 = decoder(b1, s4, int(size/1))\n",
        "  d2 = decoder(d1, s3, int(size/2))\n",
        "  d3 = decoder(d2, s2, int(size/4))\n",
        "  d4 = decoder(d3, s1, int(size/8))\n",
        "  '''outputs'''\n",
        "  outputs = Conv2D(3, (1,1), padding='same', activation='softmax')(d4)\n",
        "\n",
        "  model = Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (size, size, 3)\n",
        "  model = build_xception_unit(input_shape, size)\n",
        "  model.summary()\n",
        "model.compile(optimizer= opt, loss= iou_loss, metrics = [mean_iou, custom_f1, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "history_4 = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
        "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
        "                              epochs=epochs, verbose=1)\n",
        "\n",
        "model.save('model.h5')\n",
        "\n",
        "results_4 = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(results_4)\n",
        "\n",
        "hist_df = pd.DataFrame(history_4.history)\n",
        "hist_csv_file = 'output.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_4.history['mean_iou'])\n",
        "plt.plot(history_4.history['val_mean_iou'])\n",
        "plt.title('model accuracy mean_iou')\n",
        "plt.ylabel('mean_iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_4.history['loss'])\n",
        "plt.plot(history_4.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDYatJCdRb57"
      },
      "source": [
        "'''VGG16'''\n",
        "def conv_block(inputs, num_filters):\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2D(num_filters, 3, padding='same')(s)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  return x\n",
        "\n",
        "def decoder(inputs, skip_features, num_filters):\n",
        "  #s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs)\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_vgg16_unit(input_shape, size):\n",
        "  inputs = Input(input_shape)\n",
        "  vgg16 = VGG16(include_top = False, weights = 'imagenet', input_tensor = inputs)\n",
        "  s1 = vgg16.get_layer(\"block1_conv2\").output ##512\n",
        "  s2 = vgg16.get_layer(\"block2_conv2\").output ##256\n",
        "  s3 = vgg16.get_layer(\"block3_conv3\").output ##128\n",
        "  s4 = vgg16.get_layer(\"block4_conv3\").output ##64\n",
        "  b1 = vgg16.get_layer(\"block5_conv3\").output ##64\n",
        "  d1 = decoder(b1, s4, int(size/1))\n",
        "  d2 = decoder(d1, s3, int(size/2))\n",
        "  d3 = decoder(d2, s2, int(size/4))\n",
        "  d4 = decoder(d3, s1, int(size/8))\n",
        "  outputs = Conv2D(3, (1,1), padding='same', activation='softmax')(d4)\n",
        "\n",
        "  model = Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (size, size, 3)\n",
        "  model = build_vgg16_unit(input_shape, size)\n",
        "  model.summary()\n",
        "model.compile(optimizer= opt, loss= iou_loss, metrics = [mean_iou, custom_f1, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "history_3 = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
        "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
        "                              epochs=epochs, verbose=1)\n",
        "model.save('model.h5')\n",
        "\n",
        "results_3 = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(results_3)\n",
        "\n",
        "hist_df = pd.DataFrame(history_3.history)\n",
        "hist_csv_file = 'output.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_3.history['mean_iou'])\n",
        "plt.plot(history_3.history['val_mean_iou'])\n",
        "plt.title('model mean_iou')\n",
        "plt.ylabel('mean_iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_3.history['loss'])\n",
        "plt.plot(history_3.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFv9JRlaRqUi"
      },
      "source": [
        "'''ResNet50'''\n",
        "def conv_block(inputs, num_filters):\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2D(num_filters, 3, padding='same')(s)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  return x\n",
        "\n",
        "def decoder(inputs, skip_features, num_filters):\n",
        "  #s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs)\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_resnet50_unit(input_shape, size):\n",
        "  inputs = Input(input_shape)\n",
        "  resnet50 = ResNet50(include_top = False, weights = 'imagenet', input_tensor = inputs)\n",
        "  s1 = resnet50.get_layer(\"input_1\").output ##512\n",
        "  s2 = resnet50.get_layer(\"conv1_relu\").output ##256\n",
        "  s3 = resnet50.get_layer(\"conv2_block3_out\").output ##128\n",
        "  s4 = resnet50.get_layer(\"conv3_block4_out\").output ##64\n",
        "  b1 = resnet50.get_layer(\"conv4_block6_out\").output ##64\n",
        "  d1 = decoder(b1, s4, int(size/1))\n",
        "  d2 = decoder(d1, s3, int(size/2))\n",
        "  d3 = decoder(d2, s2, int(size/4))\n",
        "  d4 = decoder(d3, s1, int(size/8))\n",
        "  outputs = Conv2D(3, (1,1), padding='same', activation='softmax')(d4)\n",
        "  model = Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (size, size, 3)\n",
        "  model = build_resnet50_unit(input_shape, size)\n",
        "  model.summary()\n",
        "model.compile(optimizer= opt, loss= iou_loss, metrics = [mean_iou, custom_f1, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "history_2 = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
        "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
        "                              epochs=epochs, verbose=1)\n",
        "\n",
        "model.save('model.h5')\n",
        "\n",
        "results_2 = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(results_2)\n",
        "\n",
        "hist_df = pd.DataFrame(history_2.history)\n",
        "hist_csv_file = 'output.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_2.history['mean_iou'])\n",
        "plt.plot(history_2.history['val_mean_iou'])\n",
        "plt.title('model accuracy mean_iou')\n",
        "plt.ylabel('mean_iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_2.history['loss'])\n",
        "plt.plot(history_2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-e-beMR_I6"
      },
      "source": [
        "'''inceptionv3'''\n",
        "def conv_block(inputs, num_filters):\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2D(num_filters, 3, padding='same')(s)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Activation('elu')(x)\n",
        "\n",
        "  x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Activation('elu')(x)\n",
        "  \n",
        "  return x\n",
        "\n",
        "def decoder(inputs, skip_features, num_filters):\n",
        "  #s = Lambda(lambda x: x / 255) (inputs)\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs)\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_inceptionv3_unit(input_shape, size):\n",
        "  inputs = Input(input_shape)\n",
        "  inceptionv3 = InceptionV3(include_top = False, weights = 'imagenet', input_tensor = inputs)\n",
        "  s1 = inceptionv3.get_layer(\"input_1\").output ##256\n",
        "  s2 = inceptionv3.get_layer(\"activation\").output ##127\n",
        "  s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)  #128\n",
        "  s3 = inceptionv3.get_layer(\"activation_3\").output ##62\n",
        "  s3 = s3 = ZeroPadding2D((1, 1))(s3)   #64\n",
        "  s4 = inceptionv3.get_layer(\"activation_28\").output ##29\n",
        "  s4 = ZeroPadding2D(( (2, 1), (2, 1) ))(s4) \n",
        "  b1 = inceptionv3.get_layer(\"activation_74\").output ##14\n",
        "  b1 = ZeroPadding2D((1, 1))(b1)\n",
        "  d1 = decoder(b1, s4, int(size/1))\n",
        "  d2 = decoder(d1, s3, int(size/2))\n",
        "  d3 = decoder(d2, s2, int(size/4))\n",
        "  d4 = decoder(d3, s1, int(size/8))\n",
        "  outputs = Conv2D(3, (1,1), padding='same', activation='softmax')(d4)\n",
        "\n",
        "  model = Model(inputs, outputs)\n",
        "  return model\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (size, size, 3)\n",
        "  model = build_inceptionv3_unit(input_shape, size)\n",
        "  model.summary()\n",
        "model.compile(optimizer= opt, loss= iou_loss, metrics = [mean_iou, custom_f1, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "history_5 = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
        "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
        "                              epochs=epochs, verbose=1)\n",
        "\n",
        "model.save('model.h5')\n",
        "\n",
        "results_5 = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(results_5)\n",
        "\n",
        "hist_df = pd.DataFrame(history_5.history)\n",
        "hist_csv_file = 'output.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_5.history['mean_iou'])\n",
        "plt.plot(history_5.history['val_mean_iou'])\n",
        "plt.title('model accuracy mean_iou')\n",
        "plt.ylabel('mean_iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history_5.history['loss'])\n",
        "plt.plot(history_5.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}